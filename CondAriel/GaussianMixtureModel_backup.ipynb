{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model Exoplanet Atmospheric Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install umap-learn --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import delle librerie e file ARIEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "import sys\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "import taurex.log\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "import posterior_utils\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "# Importa dinamicamente il modulo helper\n",
    "import helper\n",
    "importlib.reload(helper) \n",
    "check_parameters_valid = helper.check_parameters_valid\n",
    "# Aggiungi il percorso della directory contenente helper.py\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "importlib.reload(posterior_utils)\n",
    "\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import mixture\n",
    "from helper import *\n",
    "from preprocessing import *\n",
    "from submit_format import to_competition_format\n",
    "from posterior_utils import *\n",
    "from spectral_metric import *\n",
    "from FM_utils_final import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.lines import Line2D\n",
    "from submit_format import get_unique_filename\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "taurex.log.disableLogging()\n",
    "np.set_printoptions(suppress=True, linewidth=np.nan, threshold=sys.maxsize)\n",
    "# Filtering dei Warning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"more than 1 Rp value detected in the trace! Using first value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variabili\n",
    "### Utilizziamo lo StandardScaler per trasformare e scalare gli input ottenendo *media unitaria* e *varianza nulla*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 5\n",
    "random_state = 420\n",
    "# random_state = random.randint(1,1000)\n",
    "# print(f'Random State : {random_state}')\n",
    "\n",
    "# Lettura dati dai file \n",
    "aux = np.load('aux.npy')\n",
    "spec_matrix = np.load('spectra.npy')\n",
    "noise = np.load('noise.npy')\n",
    "labels = np.load('label.npy')\n",
    "validTraces = np.load('validTraces.npy')\n",
    "num_spectra = spec_matrix.shape[0]\n",
    "labels_names = ['planet_radius','planet_temp','log_H2O','log_CO2','log_CO','log_CH4','log_NH3']\n",
    "\n",
    "# Setting dei path\n",
    "training_path = './Check_Dataset/TrainingData'\n",
    "training_GT_path = os.path.join(training_path, 'Ground Truth Package')\n",
    "trace_GT = h5py.File(os.path.join(training_GT_path, 'TraceData.hdf5'), \"r\")\n",
    "validTraces = validTraces.astype(np.int64)\n",
    "for X in trace_GT.keys():\n",
    "    tr_GT = trace_GT[X]['tracedata'][()]\n",
    "    weights_GT = trace_GT[X]['weights'][()]\n",
    "    if np.isnan(tr_GT).sum() == 1:\n",
    "        continue\n",
    "    validTraces = np.append(validTraces, int(X[12:]))\n",
    "vt = validTraces\n",
    "test_ind = np.sort(vt - 1)\n",
    "train_ind = np.setdiff1d(np.arange(num_spectra), test_ind)\n",
    "plot_ind = random.sample(range(len(test_ind)), 10)\n",
    "spectra_ind = random.sample(range(len(test_ind)), 10)\n",
    "\n",
    "#-------|Preprocessing dei dati spettrali|--------\n",
    "\n",
    "# Preprocessamento dei test spectra\n",
    "test_spectra = spec_matrix[test_ind, :]\n",
    "test_spectra = augment_data(test_spectra, noise[test_ind, :], repeat=1)\n",
    "test_spectra = test_spectra.reshape(-1, spec_matrix.shape[1])\n",
    "\n",
    "# Preprocessamento dei train spectra\n",
    "train_spectra = spec_matrix[train_ind, :]\n",
    "train_spectra = augment_data(train_spectra, noise[train_ind, :], repeat=n_repeat)\n",
    "train_spectra = train_spectra.reshape(-1, spec_matrix.shape[1])\n",
    "\n",
    "# Dati ausiliari e labels\n",
    "train_aux = aux[train_ind, :]\n",
    "train_aux = np.repeat(train_aux, repeats=n_repeat, axis=0)\n",
    "test_aux = aux[test_ind, :]\n",
    "train_labels = labels[train_ind, :]\n",
    "train_labels = np.repeat(train_labels, repeats=n_repeat, axis=0)\n",
    "test_labels = labels[test_ind, :]\n",
    "\n",
    "# Applichiamo StandardScaler per preservare meglio la distribuzione originale\n",
    "scaler = StandardScaler()\n",
    "train_spectra = scaler.fit_transform(train_spectra)\n",
    "test_spectra = scaler.transform(test_spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup per gli score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste per salvare tutti gli score\n",
    "posterior_scores = []\n",
    "spectral_scores = []\n",
    "# Inizializziamo la bounds_matrix, ovvero la matrice delle distrubuzioni a priori delle molecole\n",
    "bounds_matrix = default_prior_bounds()\n",
    "beta = 0.8\n",
    "q_list = np.linspace(0.01, 0.99, 10) # Lista di q per il calcolo del quantile\n",
    "opacity_path = \"./XSEC/\"\n",
    "CIA_path = \"./HITRAN\"\n",
    "# Carichiamo i dati dello strumento simulato di ARIEL\n",
    "ariel_wlgrid, ariel_wlwidth, ariel_wngrid, ariel_wnwidth = ariel_resolution()\n",
    "# Inizializziamo il forward model (modello planetario)\n",
    "fm = initialise_forward_model(opacity_path, CIA_path)\n",
    "RJUP = 69911000\n",
    "MJUP = 1.898e27\n",
    "RSOL = 696340000\n",
    "Rs = aux[:, 2] / RSOL\n",
    "Mp = aux[:, 4] / MJUP\n",
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applicazione del Clustering\n",
    "- $K_1$ : numero di cluster di 1o livello\n",
    "- $K_2$ : numero di 'sottocluster' (cluster di 2o livello)\n",
    "- $GMM_i$ : output del Gaussian Mixture Model\n",
    "- $Labels_i$ : labels ottenute dal GMM di ciascun livello\n",
    "> Cerchiamo, in ogni sottocluster (K2), i cluster contenenti un singolo spettro, che sono considerati *outlier*, cioe' anomali (composti da un solo spettro)\n",
    "\n",
    "`Si puo' usare il BIC (o l'AIC) per ottenere i migliori valori di $K_1$ e $K_2$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (19,10) sembra essere la migliore coppia per il clustering [Risultato BIC]\n",
    "# Dal grafico,risulta (19,15)\n",
    "n_components_range = range(10, 25) # TODO : Testare valori differenti del range\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "for n in n_components_range:\n",
    "    gmm_tmp = GaussianMixture(n_components=n,random_state=random_state,max_iter=500).fit(train_aux)\n",
    "    bic_scores.append(gmm_tmp.bic(train_aux))\n",
    "    aic_scores.append(gmm_tmp.aic(train_aux))\n",
    "# Trova il numero di componenti con il minimo BIC e AIC\n",
    "best_n_components_bic = n_components_range[np.argmin(bic_scores)]\n",
    "best_n_components_aic = n_components_range[np.argmin(aic_scores)]\n",
    "K1_b = best_n_components_bic\n",
    "K2_a = best_n_components_aic\n",
    "print(f'K1 = {K1_b}, K2 = {K2_a}')\n",
    "K1 = 19\n",
    "K2 = 15\n",
    "GMM_i = []\n",
    "Labels_i = []\n",
    "# Primo clustering (1° livello) sui dati ausiliari\n",
    "gmm = mixture.GaussianMixture(n_components=K1, random_state=random_state, max_iter=400).fit(train_aux)\n",
    "labels_1 = gmm.predict(train_aux)\n",
    "for i in range(K1):\n",
    "    spectra_i = np.where(labels_1 == i)[0]\n",
    "    print(\"Spettri nel cluster #\", i, \" -> \", len(spectra_i))\n",
    "    # Secondo clustering (2° livello) sui dati spettrali appartenenti al cluster i\n",
    "    tmp = mixture.GaussianMixture(n_components=K2, random_state=random_state).fit(train_spectra[spectra_i, :])\n",
    "    labels_2 = tmp.predict(train_spectra[spectra_i, :])\n",
    "    GMM_i.append(tmp)\n",
    "    Labels_i.append(labels_2)\n",
    "    for j in range(K2):\n",
    "        spectra_j = np.where(labels_2 == j)[0]\n",
    "        if len(spectra_j) == 1:\n",
    "            print(f\"\\t [{i}].{j} sottocluster con singolo spettro [OUTLIER]\")\n",
    "            #spectra_i = np.delete(spectra_i,spectra_j) # Rimozione dell'outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup per gli score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste per salvare tutti gli score\n",
    "posterior_scores = []\n",
    "spectral_scores = []\n",
    "# Inizializziamo la bounds_matrix, ovvero la matrice delle distrubuzioni a priori delle molecole\n",
    "bounds_matrix = default_prior_bounds()\n",
    "beta = 0.8\n",
    "q_list = np.linspace(0.01, 0.99, 10) # Lista di q per il calcolo del quantile\n",
    "opacity_path = \"./XSEC/\"\n",
    "CIA_path = \"./HITRAN\"\n",
    "# Carichiamo i dati dello strumento simulato di ARIEL\n",
    "ariel_wlgrid, ariel_wlwidth, ariel_wngrid, ariel_wnwidth = ariel_resolution()\n",
    "# Inizializziamo il forward model (modello planetario)\n",
    "fm = initialise_forward_model(opacity_path, CIA_path)\n",
    "RJUP = 69911000\n",
    "MJUP = 1.898e27\n",
    "RSOL = 696340000\n",
    "Rs = aux[:, 2] / RSOL\n",
    "Mp = aux[:, 4] / MJUP\n",
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo degli score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcoliamo lo score solo su una parte dei sample (fino a 2*spec_max)\n",
    "spec_max = 20\n",
    "n_spec = 0\n",
    "spec_ind = random.sample(range(len(test_ind)),spec_max*2)\n",
    "# Liste per salvare i parametri usati e gli score ottenuti\n",
    "'''\n",
    "k1_vals = []\n",
    "k2_vals = []\n",
    "posterior_scores_k = []\n",
    "spectral_scores_k = []\n",
    "scores_k = []\n",
    "'''\n",
    "for X in range(len(test_ind)):\n",
    "\t# Fitting sui dati ausiliari\n",
    "\tidx1 = gmm.predict(test_aux[X, :].reshape(1, -1))[0]\n",
    "\tkm = GMM_i[idx1] \n",
    "\tlabels_2 = Labels_i[idx1]\n",
    "\t# Fitting sui dati spettrali\n",
    "\tidx2 = km.predict(test_spectra[X, :].reshape(1, -1))[0]\n",
    "\tidx_1 = np.where(labels_1 == idx1)[0]\n",
    "\tidx_2 = np.where(labels_2 == idx2)[0]\n",
    "\tcluster_membs = idx_1[labels_2==idx2] # Indici dei membri del cluster\n",
    "\t# Estraiamo i membri del cluster\n",
    "\tlab = train_labels[cluster_membs,:]\n",
    "\tlab = lab.reshape(-1,lab.shape[-1])\n",
    "\tposterior = lab\n",
    "\t# Calcoliamo il peso di ogni membro del cluster\n",
    "\tweights1 = np.ones((posterior.shape[0],1)) / posterior.shape[0]\n",
    "\tplanet_index = test_ind[X]+1\n",
    "\t# Carichiamo i dati del ground truth\n",
    "\ttr_GT = trace_GT[f'Planet_train{planet_index}']['tracedata'][()]\n",
    "\twh_GT = trace_GT[f'Planet_train{planet_index}']['weights'][()]\n",
    "\t# Controllo se ci sono nan -> se ci sono, non calcoliamo lo score\n",
    "\tif np.isnan(tr_GT).sum() >= 1:\n",
    "\t\tprint(\"nan trovato\")\n",
    "\t\texit()\n",
    "\tscore = compute_posterior_loss(posterior, weights1, tr_GT, wh_GT,bounds_matrix)\n",
    "\tposterior_scores.append(score)\n",
    "\tif n_spec<spec_max and X in spec_ind:\n",
    "\t\ttry:\n",
    "\t\t\tproxy_compute_spectrum = setup_dedicated_fm(fm, test_ind[X], Rs, Mp, ariel_wngrid, ariel_wnwidth )\n",
    "\t\t\tsscore = compute_spectral_loss(posterior, weights1, tr_GT, wh_GT, bounds_matrix, proxy_compute_spectrum, q_list)\n",
    "\t\t\tspectral_scores.append(sscore)\n",
    "\t\t\tprint('*****Spectral: ',sscore)\n",
    "\t\t\tprint('*****Posterior: ',score)\n",
    "\t\t\tn_spec+=1\n",
    "\t\texcept RuntimeWarning:\n",
    "\t\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcolo degli score medi\n",
    "- Posterior Score : 80%\n",
    "- Spectral Score : 20%\n",
    "- Final score = Score finale per la Leaderboard\n",
    "> final_score = 0.8 * posterior_score + 0.2 * spectral_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_posterior_score = np.mean(posterior_scores)\n",
    "print(f'Posterior_Score: {avg_posterior_score}')\n",
    "avg_spectral_score = np.mean(spectral_scores)\n",
    "print(f'Spectral_Score: {avg_spectral_score}')\n",
    "final_score = (1 - beta) * avg_spectral_score + beta * avg_posterior_score\n",
    "print(f\"final score: {final_score:.4f}\")    \n",
    "# Salvataggio risultati\n",
    "'''\n",
    "k1_vals.append(K1)\n",
    "k2_vals.append(K2)\n",
    "posterior_scores_k.append(avg_posterior_score)\n",
    "spectral_scores_k.append(avg_spectral_score)\n",
    "scores_k.append(final_score)\n",
    "with open(\"results.txt\", \"a\") as f:\n",
    "    f.write(\"\\n------------------------------\\n\")\n",
    "    f.write(\"K1: {}\\n\".format(k1_vals))\n",
    "    f.write(\"K2: {}\\n\".format(k2_vals))\n",
    "    f.write(\"Posterior scores: {}\\n\".format(posterior_scores_k))\n",
    "    f.write(\"Spectral scores: {}\\n\".format(spectral_scores_k))\n",
    "    f.write(\"Final scores: {}\\n\".format(scores_k))\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting grafico Score/Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "score_plot_ind = 0\n",
    "# Leggi l'intero contenuto del file di testo\n",
    "with open(\"results.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "# Suddividi in blocchi\n",
    "blocks = [block for block in content.split('------------------------------') if block.strip()]\n",
    "# Liste per accumulare i dati di TUTTE le iterazioni\n",
    "k1_values = []\n",
    "k2_values = []\n",
    "posterior_vals = []\n",
    "spectral_vals = []\n",
    "final_vals = []\n",
    "# Funzione per fare il parsing di un valore in modo sicuro\n",
    "def safe_literal_eval(value):\n",
    "    try:\n",
    "        value = value.strip().lower()\n",
    "        if value == \"nan\":\n",
    "            return np.nan\n",
    "        return ast.literal_eval(value)\n",
    "    except (SyntaxError, ValueError):\n",
    "        print(f\"Errore nel parsing: {value}\")\n",
    "        return None\n",
    "\n",
    "# Parsing di tutti i blocchi\n",
    "for block in blocks:\n",
    "    k1, k2 = None, None\n",
    "    posterior = spectral = final = None\n",
    "    lines = [line.strip() for line in block.strip().splitlines() if line.strip()]\n",
    "    for line in lines:\n",
    "        if line.startswith(\"K1:\"):\n",
    "            k1 = safe_literal_eval(line.split(\":\", 1)[1].strip())\n",
    "        elif line.startswith(\"K2:\"):\n",
    "            k2 = safe_literal_eval(line.split(\":\", 1)[1].strip())\n",
    "        elif line.startswith(\"Posterior\"):\n",
    "            posterior = safe_literal_eval(line.split(\":\", 1)[1].strip())\n",
    "        elif line.startswith(\"Spectral\"):\n",
    "            spectral = safe_literal_eval(line.split(\":\", 1)[1].strip())\n",
    "        elif line.startswith(\"Final\"):\n",
    "            final = safe_literal_eval(line.split(\":\", 1)[1].strip())\n",
    "    \n",
    "    # Se tutti i valori sono validi, registra una riga\n",
    "    if all(v is not None for v in [k1, k2, posterior, spectral, final]):\n",
    "        k1_values.append(k1)\n",
    "        k2_values.append(k2)\n",
    "        # Se posterior/spectral/final sono array/list, fai la media\n",
    "        posterior_vals.append(np.mean(posterior) if isinstance(posterior, (list, tuple, np.ndarray)) else posterior)\n",
    "        spectral_vals.append(np.mean(spectral) if isinstance(spectral, (list, tuple, np.ndarray)) else spectral)\n",
    "        final_vals.append(np.mean(final) if isinstance(final, (list, tuple, np.ndarray)) else final)\n",
    "\n",
    "# Plot con coppie (K1,K2)\n",
    "k_pairs = [f\"({k1},{k2})\" for k1, k2 in zip(k1_values, k2_values)]\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = np.arange(len(k_pairs))\n",
    "plt.plot(x_pos, posterior_vals, marker='o', color='red', label='Posterior Score', linestyle='--')\n",
    "plt.plot(x_pos, spectral_vals, marker='x', color='black', label='Spectral Score', linestyle='-.')\n",
    "plt.plot(x_pos, final_vals, marker='*', color='blue', label='Final Score', linestyle=':')\n",
    "plt.xticks(x_pos, k_pairs, rotation=45, ha='right', fontsize=8)\n",
    "plt.xlabel('(K1, K2)', fontweight='bold')\n",
    "plt.ylabel('Scores', fontweight='bold')\n",
    "plt.title('Confronto Scores per Configurazioni (K1,K2)', pad=20)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Plot dei punti\n",
    "for i, (p, s, f) in enumerate(zip(posterior_vals, spectral_vals, final_vals)):\n",
    "    plt.annotate(f'{p:.1f}', (x_pos[i], p), textcoords=\"offset points\", xytext=(0,5), ha='center', color='red', fontsize=8)\n",
    "    plt.annotate(f'{s:.1f}', (x_pos[i], s), textcoords=\"offset points\", xytext=(0,5), ha='center', color='black', fontsize=8)\n",
    "    plt.annotate(f'{f:.1f}', (x_pos[i], f), textcoords=\"offset points\", xytext=(0,5), ha='center', color='blue', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plot_fname = get_unique_filename('Grafico','.png','./Cluster_Scores/')\n",
    "plt.savefig(plot_fname)\n",
    "plt.show()\n",
    "# Scrittura su CSV dei risultati\n",
    "'''\n",
    "df_results = pd.DataFrame({\n",
    "    'K1': k1_values,\n",
    "    'K2': k2_values,\n",
    "    'PosteriorScore': posterior_vals,\n",
    "    'SpectralScore': spectral_vals,\n",
    "    'FinalScore': final_vals\n",
    "})\n",
    "# Salva su CSV\n",
    "df_results.to_csv('results.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafico andamento clustering\n",
    "### Fissato K1, il grafico indica l'andamento del final_score per il variare di K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------|PLOT FINAL SCORE|------------#\n",
    "df = pd.read_csv('results.csv')\n",
    "# Rinomina delle colonne (se necessario)\n",
    "df.columns = ['K1', 'K2', 'PosteriorScore', 'SpectralScore', 'FinalScore']\n",
    "#-----Rimuovi le parentesi quadre e converti K1 e K2 in interi\n",
    "df['K1'] = df['K1'].str.replace(r'[\\[\\]]', '', regex=True).astype(int)\n",
    "df['K2'] = df['K2'].str.replace(r'[\\[\\]]', '', regex=True).astype(int)\n",
    "#-----Filtraggio cluster a singolo sottocluster\n",
    "# df = df[~((df['K2'] == 1) & (df['K1'] != 1))]  # esclude le righe dove K2 è 1 e K1 non è 1\n",
    "df = df[df['K2'] != 1]  # esclude le righe dove K2 è 1\n",
    "# Ordina il dataframe per K1 e K2\n",
    "df = df.sort_values(by=['K1', 'K2'])\n",
    "k1_values = sorted(df['K1'].unique())\n",
    "unique_k2 = sorted(df['K2'].unique())\n",
    "colormap = plt.cm.get_cmap('tab10', len(k1_values)+2)\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, k1 in enumerate(k1_values):\n",
    "    subset = df[df['K1'] == k1].sort_values(by='K2')\n",
    "    plt.plot(subset['K2'], subset['FinalScore'], marker='o', label=f'K1={k1}', color=colormap(i))\n",
    "plt.xticks(unique_k2)\n",
    "plt.xlabel('K2')\n",
    "plt.ylabel('Final Score')\n",
    "plt.title('Andamento final_score')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "y_min, y_max = plt.ylim()  # prendi i limiti correnti\n",
    "margine = (y_max - y_min) * 0.3  # aggiunge un margine del 10%\n",
    "plt.ylim(y_min - margine, y_max + margine)\n",
    "final_fname = get_unique_filename(f'Grafico_Final_{K1}-{K2}', '.png', directory='./Grafici_Clustering')\n",
    "plt.savefig(final_fname)\n",
    "plt.show()\n",
    "\n",
    "#------------|PLOT SPECTRAL SCORE|------------#\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, k1 in enumerate(k1_values):\n",
    "    subset = df[df['K1'] == k1].sort_values(by='K2')\n",
    "    plt.plot(subset['K2'], subset['SpectralScore'], marker='x', label=f'K1={k1}', color=colormap(i))\n",
    "plt.xticks(unique_k2)\n",
    "plt.xlabel('K2')\n",
    "plt.ylabel('Spectral Score')\n",
    "plt.title('Andamento spectral_score')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "y_min, y_max = plt.ylim()  # prendi i limiti correnti\n",
    "margine = (y_max - y_min) * 0.3  # aggiunge un margine del 10%\n",
    "plt.ylim(y_min - margine, y_max + margine)\n",
    "spec_fname = get_unique_filename(f'Grafico_Spectral_{K1}-{K2}', '.png', directory='./Grafici_Clustering')\n",
    "plt.savefig(spec_fname)\n",
    "plt.show()\n",
    "\n",
    "#------------|PLOT POSTERIOR SCORE|------------#\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, k1 in enumerate(k1_values):\n",
    "    subset = df[df['K1'] == k1].sort_values(by='K2')\n",
    "    plt.plot(subset['K2'], subset['PosteriorScore'], marker='o', label=f'K1={k1}', color=colormap(i))\n",
    "plt.xticks(unique_k2)\n",
    "plt.xlabel('K2')\n",
    "plt.ylabel('Posterior Score')\n",
    "plt.title('Andamento posterior_score')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.grid(True)\n",
    "y_min, y_max = plt.ylim()  # prendi i limiti correnti\n",
    "margine = (y_max - y_min) * 0.3  # aggiunge un margine del 10%\n",
    "plt.ylim(y_min - margine, y_max + margine)\n",
    "post_fname = get_unique_filename(f'Grafico_Posterior_{K1}-{K2}', '.png', directory='./Grafici_Clustering')\n",
    "plt.savefig(post_fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting delle distribuzioni\n",
    "### Si utiliza 'corner' per il plot delle distribuzioni bayesiane a posteriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plottiamo le distribuzioni dei `plot_ind` spettri\n",
    "for X in plot_ind:\n",
    "\t# Predizione sui dati ausiliari di test (1o livello)\n",
    "\tidx1 = gmm.predict(test_aux[X,:].reshape(1,-1))[0]\n",
    "\tkm = GMM_i[idx1] # Selezione etichette\n",
    "\tlabels_2 = Labels_i[idx1] \n",
    "\t# Predizione sui dati spettrali di test (2o livello)\n",
    "\tidx2 = km.predict(test_spectra[X,:].reshape(1,-1))[0]\n",
    "\tidx_1 = np.where(labels_1 == idx1)[0] # Individuazione indici nel cluster\n",
    "\tidx_2 = np.where(labels_2 == idx2)[0] # Individuazione indici nel cluster\n",
    "\tlab = train_labels[idx_1[idx_2],:] \n",
    "\tmean_lab = np.mean(lab,axis=0) # Media delle predizioni per il plot\n",
    "\tmean_GT = np.average(tr_GT,axis=0,weights=wh_GT) # Valore medio pesato dei dati reali\n",
    "\t# Lettura dati Ground Truth\n",
    "\ttr_GT = trace_GT[f'Planet_train{test_ind[X]+1}']['tracedata'][()]\n",
    "\twh_GT = trace_GT[f'Planet_train{test_ind[X]+1}']['weights'][()]\n",
    "\t# Creazione della figura e plot dei dati reali\n",
    "\tfigure = corner.corner(tr_GT,quiet=True) #,weights=wh_GT\n",
    "\taxes = np.array(figure.axes).reshape((tr_GT.shape[1], tr_GT.shape[1]))\n",
    "\t# Creazione della legenda\n",
    "\tlegend_elements = [\n",
    "        Line2D([0], [0], color='blue', lw=2, label='Distribuzione Reale (tr_GT)'),\n",
    "        Line2D([0], [0], color='red', lw=2, label='Distribuzione Predetta (lab)'),\n",
    "        Line2D([0], [0], color='green', lw=2, label='Valore Vero (test_labels)'),\n",
    "        Line2D([0], [0], color='black', lw=2, label='Media Predetta (mean_lab)')\n",
    "    ]\n",
    "\t# Plot delle distribuzioni (reali)\n",
    "\tfor i in range(tr_GT.shape[1]):\n",
    "\t\tax = axes[i, i]\n",
    "\t\tax.sharex(axes[tr_GT.shape[1]-1,i])\n",
    "\t\tax.axvline(test_labels[X,i], color=\"g\",lw=2)\n",
    "\t\tax.axvline(mean_lab[i], color=\"r\",lw=1.5)\n",
    "\t\tax.axvline(mean_GT[i],color=\"blue\",lw=2)\n",
    "\t\t#ax.axvspan(mean_lab[i] - std_lab[i], mean_lab[i] + std_lab[i], alpha=0.3, color='red')\n",
    "\t\tax.relim()\n",
    "\t\tax.autoscale()\n",
    "\t\tax.set_title(labels_names[i])\n",
    "\tfigure.legend(\n",
    "        handles=legend_elements, \n",
    "        loc='upper right', \n",
    "        bbox_to_anchor=(0.92, 0.92),\n",
    "        frameon=True,\n",
    "        framealpha=0.9\n",
    "    )\n",
    "\tfigure.set_figheight(8.5)\n",
    "\tfigure.set_figwidth(12)\n",
    "\t# Plot delle predizioni\n",
    "\tcorner.corner(lab,fig=figure,quiet=True, color='red')\n",
    "\tfigure.savefig('./GMM_plots/Planet_'+str(test_ind[X]+1)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvataggio risultati\n",
    "### Salviamo i risultati mediante `to_competition_format()`, nel formato richiesto dalla Challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = to_competition_format(lab,weights1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
